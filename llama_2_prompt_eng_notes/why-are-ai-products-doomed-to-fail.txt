With the apparition of Generative AI and the impressive performance that came with it, most companies revised their strategy to get AI in their product.

In addition, start-ups with the term “AI” embedded in their name have been emerging in all domains with one main goal: finding a problem that GenAI could solve.

As a freelancer, I have been advising companies and developing features using LLMs in various domains: HR, sales, short-term rents, AI start-up, …

In this article, I share my raw experience as an NLP engineer during this hype and my opinion on mistakes I see companies making.

The story of an NLP engineer through the AI hype
“GPT-3.5’s here. We need to surf the wave!”
I started being involved in LLMs back in January 2023.

However, after several discussions with users within the company, I noticed some limitations of the tool: hallucinations, usage drift, or context parsing inaccuracies…

I planned to work alongside the users in building a tool capable of assisting them. With constant iterations between users and development, it was possible to create a tool specifically designed to solve their pain.

“We need to show to stakeholders what we can achieve with AI! But it has to be secret to increase the “Wahou” effect!”…, told me my managers.

Soon after, I was already developing another chatbot with the same frameworks, this time for HR. My colleagues developed a feature using Whisper. Another a storyteller for children with images generated with Dalle-2.

4 months of learning & discussion with companies
Even though I deeply disagreed with how LLM projects were managed, I was able to see the value of this technology.

This feature remains one of the principal applications of LLMs these days, popularized by RAG (Retrieval-Augmented Generation) as the “hello world” of LLMs.

In reality, developing this feature is way more complex than what social media and other courses show: various data sources, data pipelines to ensure the latest information is retrieved, fine-tuning, serving, …

Fit Your LLM in a single GPU with Gradient Checkpointing, LoRA, and Quantization.
Whoever has ever tried to fine-tune a Large Language Model knows how hard it is to handle the GPU memory.
pub.towardsai.net

4 months of intense learning and experimentation.

Automate short-term rental owners' responses to their guests
During this project, I was in charge of building a response-generation engine to help short-term rental owners answer their guests’ questions.

The owner’s data was stored in a PostgreSQL database and was diverse: availability dates, number of guests allowed, house equipment list, descriptions… In other words, using a vector database coupled with RAG techniques only solved a part of the problem.

I had to find a way to find the correct source of information for the guest’s question, in addition to embedding relevant texts like the house’s description.

The majority of their code was written in .Net. For this feature, they built a prototype by calling OpenAI API (in .Net) and retrieving information from the database using a custom API that threw all the information of a guest and his accommodation as a context for the LLM.

However, the results were not as good as expected: inaccuracies, prompt reaching the context size limit, etc…

I remember reviewing the users’ feedback on the prototype, and it was far from favorable. There was massive room for improvement! The need was there and I was able to help.

But something caught my attention on the first day of the mission. My manager, the company co-founder, who has been CTO for 11 years, decided to change his role to… Chief AI.

During the project, we rapidly understood that using OpenAI was one thing, but making the text generation engine tool work along their tech stack and data was a real challenge: a part of the data had to be embedded, while the other retrieved using SQL queries generated by an LLM agent. Therefore, we needed to set up a router for retrieving the right data depending on the guest’s question.

In addition, the fact the entire code was written in .Net didn’t simplify the transition.

At this moment, everybody understood that implementing AI was a really difficult task, requiring coordination and important investment upfront.

Calling OpenAI and doing some prompt engineering was just the tip of the iceberg. Having a data architecture ready to handle this new feature was the real challenge, in addition to later work on improving the model(s) outputs.

Seeing that, it didn’t take long for me to be removed from the project. The reason: “they needed to iterate over the prototype they already built”.

Two weeks later, I got reached out by a serial entrepreneur from the US. He wanted me to help him and his CTO build a product that leverages the summarizing power of LLMs to facilitate the decision-making process within large communities.

The project sounded good, and everything had to be built from scratch. “I can guide them through the process while learning LLMOps principles at the same time”, I thought.

During the two first weeks, I built all the architecture along the back-end and front developers. Together, we achieved a prototype that was deployed in production in a record time!

However, during the project, once a feature was deployed in production, it was time to develop a new “shiny” feature “to do like every other AI product”.

But at the same time, I had to explain to the stakeholders why the current outputs in production were not that great…
In other words, I had been building many V1s, but none of them would have made the product stand out from another “GPT-wrapper”-SAAS.